{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretable Maching Learning models and Energy Storage Systems at the Southwest Power Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPP_path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)'\n",
    "\n",
    "def add_info(df):\n",
    "    intervals = df[df.columns[0]].values.tolist()\n",
    "    dates = []\n",
    "    times = []\n",
    "    weekday = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hour = []\n",
    "    minute_of_day = []\n",
    "    for interval in intervals:\n",
    "        date = interval.split(' ')[0]\n",
    "        try:\n",
    "            date = dt.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        except:\n",
    "            date = dt.datetime.strptime(date,'%m/%d/%Y').date()            \n",
    "        dates.append(date)\n",
    "        months.append(date.month)\n",
    "        days.append(date.day)\n",
    "        if date.weekday() < 5:\n",
    "            weekday.append(True)\n",
    "        else:\n",
    "            weekday.append(False)\n",
    "        time = interval.split(' ')[1].split('.')[0]\n",
    "        time = dt.datetime.strptime(time,'%H:%M:%S').time()\n",
    "        times.append(time)\n",
    "        hour.append(dt.time(time.hour))\n",
    "        minute_of_day.append(60*time.hour+time.minute)\n",
    "    df['Local Date'] = np.array(dates)\n",
    "    df['Local Time'] = np.array(times)\n",
    "    df['Hour'] = np.array(hour)\n",
    "    df['Weekday'] = np.array(weekday)\n",
    "    df['Month'] = np.array(months)\n",
    "    df['Day'] = np.array(days)\n",
    "    df['Minute of Day'] = np.array(minute_of_day)\n",
    "    return df\n",
    "\n",
    "def GMT2CT(s):\n",
    "    date = s.split('T')[0]\n",
    "    date = dt.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "    time = s.split('T')[1][:-1]\n",
    "    hour = int(time.split(':')[0])\n",
    "    if hour >= 6:\n",
    "        hour = hour - 6\n",
    "    else:\n",
    "        hour = 24 + (hour - 6)\n",
    "        date = date - timedelta(1)\n",
    "    time = str(hour) + ':' + time.split(':')[1] + ':' + time.split(':')[2]\n",
    "    time = dt.datetime.strptime(time,'%H:%M:%S').time()\n",
    "    return [date, time]\n",
    "\n",
    "def ssr(prediction, test):\n",
    "    return ((prediction - test)**2).sum()\n",
    "\n",
    "def pe(prediction, test):\n",
    "    return (abs((prediction - test)/test))*100\n",
    "\n",
    "def filler(df, DA):\n",
    "    price_list = df[DA].values.tolist()\n",
    "    value = 0.0\n",
    "    new_list = []\n",
    "    for price in price_list:\n",
    "        if math.isnan(price)==True:\n",
    "            new_list.append(value)\n",
    "        else:\n",
    "            value = price\n",
    "            new_list.append(price)\n",
    "    df[DA] = np.array(new_list)\n",
    "    return df\n",
    "\n",
    "def means_dict(d):\n",
    "    means_d = {}\n",
    "    for key in d.keys():\n",
    "        means_d[key] = np.nanmean(np.array(d[key]))\n",
    "    return means_d\n",
    "\n",
    "def means_std_dict(d):\n",
    "    means_d = {}\n",
    "    for key in d.keys():\n",
    "        means_d[key] = [round(np.array(d[key]).mean(),6),round(stats.sem(np.array(d[key])),6)]\n",
    "    return means_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mix_2018 = pd.read_csv(SPP_path + '\\Generation Mix By Fuel Type\\GenMix_2018.csv')\n",
    "list_intervals = gen_mix_2018[gen_mix_2018.columns[0]].values.tolist()\n",
    "local_time = []\n",
    "local_date = []\n",
    "for value in list_intervals:\n",
    "    local_date.append(GMT2CT(value)[0])\n",
    "    local_time.append(GMT2CT(value)[1])\n",
    "gen_mix_2018['Local Date'] = np.array(local_date)\n",
    "gen_mix_2018['Local Time'] = np.array(local_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_31 = []\n",
    "for n in range(1,10):\n",
    "    days_31.append('0'+str(n))\n",
    "for n in range(10,32):\n",
    "    days_31.append(str(n))\n",
    "cal_dict = {'01':days_31,\n",
    "            '02':days_31[0:28],\n",
    "            '03':days_31,\n",
    "           '04':days_31[0:-1],\n",
    "            '05':days_31,\n",
    "            '06':days_31[0:-1],\n",
    "           '07':days_31,\n",
    "            '08':days_31,\n",
    "            '09':days_31[0:-1],\n",
    "           '10':days_31,\n",
    "           '11':days_31[0:-1],\n",
    "           '12':days_31}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and concatenating RT datasets\n",
    "\n",
    "# run time: ~ 9 min\n",
    "\n",
    "path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)\\RT\\2018'\n",
    "RT_path = 'RTBM-LMP-DAILY-SL-2018'\n",
    "end = '.csv'\n",
    "dfs = []\n",
    "for key in cal_dict.keys():\n",
    "    for value in cal_dict[key]:\n",
    "        dfs.append(pd.read_csv(path+'\\\\'+key+'\\\\By_Day\\\\'+RT_path+key+value+end))\n",
    "RT = pd.concat(dfs)\n",
    "print('Loaded RT data')\n",
    "\n",
    "# Aggregating settlement location for system-wide data\n",
    "RT2018_aggloc = RT.groupby('Interval')[['Interval',' LMP']].agg({'Interval':'first',\n",
    "                                                                    ' LMP':'mean'})\n",
    "RT2018 = add_info(RT2018_aggloc)\n",
    "RT2018 = RT2018.rename(columns={' LMP':'LMP'})\n",
    "RT2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and concatenating DA datasets\n",
    "\n",
    "# run time: ~ 2 min\n",
    "\n",
    "path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)\\DA\\2018'\n",
    "DA_path = 'DA-LMP-SL-2018'\n",
    "end = '0100.csv'\n",
    "dfs = []\n",
    "for key in cal_dict.keys():\n",
    "    for value in cal_dict[key]:\n",
    "        dfs.append(pd.read_csv(path+'\\\\'+key+'\\\\By_Day\\\\'+DA_path+key+value+end))\n",
    "DA2018 = pd.concat(dfs)\n",
    "print('Loaded DA data')\n",
    "\n",
    "# Aggregating settlement location for system-wide data\n",
    "DA2018_aggloc = DA2018.groupby('Interval')[['Interval','LMP']].agg({'Interval':'first',\n",
    "                                                                    'LMP':'mean'})\n",
    "DA2018_aggloc = add_info(DA2018_aggloc)\n",
    "DA2018_aggloc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging RT, DA and Load data\n",
    "comparison = DA2018_aggloc.iloc[:,0:2].join(RT2018,how='right',lsuffix='_DA',rsuffix='_RT')\n",
    "DART2018_5min = filler(comparison, 'LMP_DA')\n",
    "gen_2018 = gen_mix_2018\n",
    "new_index = []\n",
    "for i in range(gen_2018.shape[0]):\n",
    "    new_index.append(gen_2018['Local Date'].iloc[i].strftime(\"%m/%d/%Y\") + ' ' + gen_2018['Local Time'].iloc[i].strftime(\"%H:%M:%S\"))\n",
    "gen_2018.index = np.array(new_index)\n",
    "DART_gen_2018 = gen_2018.join(DART2018_5min,how='right',lsuffix='_gen',rsuffix='_price')\n",
    "DART_gen_2018 = DART_gen_2018.rename(columns={' Average Actual Load':'Load',\n",
    "                                             ' Wind Self':'Wind',\n",
    "                                             ' Coal Market':'Coal_Mkt',\n",
    "                                             ' Coal Self':'Coal_Self',\n",
    "                                             'Local Time_price':'Local Time',\n",
    "                                             'Local Date_price':'Local Date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening and concatenating DA Load datasets\n",
    "path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)\\DA Load\\2018'\n",
    "DA_path = 'DA-MC-2018'\n",
    "end = '0100.csv'\n",
    "dfs = []\n",
    "for key in cal_dict.keys():\n",
    "    for value in cal_dict[key]:\n",
    "        dfs.append(pd.read_csv(path+'\\\\'+key+'\\\\'+DA_path+key+value+end))\n",
    "DA_load_2018 = pd.concat(dfs)\n",
    "\n",
    "# Merging Forecast Load to dataset\n",
    "DA_load_2018.index = DA_load_2018['Interval']\n",
    "comparison = DA_load_2018.iloc[:,8:9].join(DART2018,how='right')\n",
    "DART2018 = filler(comparison, ' Total Demand')\n",
    "DART2018 = DART2018.iloc[11:,:]\n",
    "DART2018 = DART2018.rename(columns={' Total Demand':'DA Load'})\n",
    "DART2018['Load_Diff'] = DART2018['Load'] - DART2018['DA Load']\n",
    "\n",
    "DART2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding previous-interval and moving averages\n",
    "DART2018_1 = DART2018.iloc[1:,:]\n",
    "previous_load_diff = DART2018['Load_Diff'].iloc[0:-1].values\n",
    "DART2018_1['Previous_Load_Diff'] = previous_load_diff\n",
    "previous_RT = DART2018['LMP_RT'].iloc[0:-1].values\n",
    "DART2018_1['Previous_RT'] = previous_RT\n",
    "\n",
    "previous_spread = DART2018['Spread'].iloc[0:-1].values\n",
    "DART2018_1['Previous_Spread'] = previous_spread\n",
    "\n",
    "step = 3\n",
    "previous_RT = DART2018_1['LMP_RT'].iloc[0:(DART2018_1.shape[0] - step)].values\n",
    "previous_means_RT = np.empty(previous_RT.size)\n",
    "previous_load = DART2018_1['Load_Diff'].iloc[0:(DART2018_1.shape[0] - step)].values\n",
    "previous_means_load = np.empty(previous_load.size)\n",
    "for i in range(DART2018_1.shape[0] - step):\n",
    "    previous_means_RT[i] = previous_RT[i:i+step].mean()\n",
    "    previous_means_load[i] = previous_load[i:i+step].mean()\n",
    "DART2018_2 = DART2018_1.iloc[step:,:]\n",
    "DART2018_2['Previous_RT_2'] = previous_means_RT\n",
    "DART2018_2['Previous_Load_Diff_2'] = previous_means_load\n",
    "\n",
    "DART2018_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_clusters_2(train_df, depth, plot):\n",
    "    # converting datetime to int\n",
    "    minutes = []\n",
    "    for time in train_df.index:\n",
    "        minutes.append(time.hour * 60 + time.minute)\n",
    "    train_df['minutes'] = np.array(minutes)\n",
    "    X = train_df['minutes'].values.reshape(-1,1)\n",
    "    y = train_df['LMP_RT']\n",
    "    # Fit regression model\n",
    "    regr_1 = DecisionTreeRegressor(max_depth=depth)\n",
    "    regr_1.fit(X, y)\n",
    "    # Predict\n",
    "    X_test = train_df['minutes'].values.reshape(-1,1)\n",
    "    y_1 = regr_1.predict(X_test)\n",
    "    train_df['sklearn cluster'] = y_1\n",
    "    price_leaves = []\n",
    "    for price in y_1:\n",
    "        if price not in price_leaves:\n",
    "            price_leaves.append(price)\n",
    "    branch_dfs = []\n",
    "    for price in price_leaves:\n",
    "        branch_dfs.append(train_df[train_df['sklearn cluster']==price])\n",
    "    if plot == True:\n",
    "        plt.plot(X_test, y_1, color=\"cornflowerblue\",label=\"max_depth=\"+str(depth), linewidth=2)\n",
    "        plt.legend()\n",
    "    return branch_dfs\n",
    "\n",
    "def crit_pts2(df):\n",
    "    intervals = df.index.values.tolist()\n",
    "    i = 0\n",
    "    points = []\n",
    "    for time in intervals:\n",
    "        mean = df['LMP_RT'][0:i+1].mean()\n",
    "        if df['LMP_RT'][i] > 2*mean:\n",
    "            if (df['LMP_RT'][i] - mean) > df['LMP_RT'].mean()/3:\n",
    "                if df['LMP_RT'][i] > 1.6*df['LMP_DA'][i]:\n",
    "                    points.append(time)\n",
    "        elif df['LMP_RT'][i] > 1.6*df['LMP_DA'][i]:\n",
    "            points.append(time)\n",
    "        i += 1\n",
    "    x = np.array(points)\n",
    "    \n",
    "    i = 0\n",
    "    LMP = []\n",
    "    for time in df.index:\n",
    "        if time in x:\n",
    "            LMP.append(df['LMP_RT'][i])\n",
    "        i += 1\n",
    "    y = np.array(LMP)\n",
    "    \n",
    "    return [x,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 2 years for the 1st time\n",
    "\n",
    "np.seterr(divide='print',invalid='print')\n",
    "\n",
    "n = 10\n",
    "PE_10_list = []\n",
    "errors = 0\n",
    "\n",
    "df = DART2018_2\n",
    "\n",
    "slopes = {}\n",
    "\n",
    "r2 = []\n",
    "r2adj = []\n",
    "for i in range(n):\n",
    "    for j in range(1,13):\n",
    "        by_month = df[df['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.5)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Load_Diff', 'Previous_Load_Diff',\n",
    "                                                  'Previous_RT', 'Previous_Load_Diff_2', 'Previous_RT_2']].mean()\n",
    "        train_data['RT_std'] = train.groupby('Local Time')['LMP_RT'].std()\n",
    "        train_data['DA_std'] = train.groupby('Local Time')['LMP_DA'].std()        \n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Load_Diff', 'Previous_Load_Diff', \n",
    "                                                'Previous_RT', 'Previous_Load_Diff_2', 'Previous_RT_2']].mean()\n",
    "        test_data['RT_std'] = test.groupby('Local Time')['LMP_RT'].std()\n",
    "        test_data['DA_std'] = test.groupby('Local Time')['LMP_DA'].std() \n",
    "        test_data_copy = test_data.copy()\n",
    "        \n",
    "        test_data_copy['fitted RT (10)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters_2(train_data, 2, False)\n",
    "        for cluster in branch_dfs:\n",
    "            try:\n",
    "                result10 = smf.ols(formula=\"\"\"LMP_RT ~ LMP_DA + RT_std + DA_std + Load + Wind + Load_Diff + Previous_Load_Diff\n",
    "                                    + Previous_Load_Diff_2 + Previous_RT + Previous_RT_2\"\"\", data=cluster).fit()\n",
    "                r2.append(result10.rsquared)\n",
    "                r2adj.append(result10.rsquared_adj)\n",
    "                for var in result10.params.index:\n",
    "                    if var not in slopes.keys():\n",
    "                        slopes[var] = []\n",
    "                        slopes[var].append(result10.params.loc[var])\n",
    "                    else:\n",
    "                        slopes[var].append(result10.params.loc[var])\n",
    "                for hour in cluster.index:\n",
    "                    test_data_copy['fitted RT (10)'].loc[hour] = (\n",
    "                                                test_data_copy['Previous_RT_2'].loc[hour]*result10.params[10] +  \n",
    "                                                test_data_copy['Previous_RT'].loc[hour]*result10.params[9] +\n",
    "                                                test_data_copy['Previous_Load_Diff_2'].loc[hour]*result10.params[8] + \n",
    "                                                test_data_copy['Previous_Load_Diff'].loc[hour]*result10.params[7] + \n",
    "                                                test_data_copy['Load_Diff'].loc[hour]*result10.params[6] + \n",
    "                                                test_data_copy['Wind'].loc[hour]*result10.params[5] + \n",
    "                                                test_data_copy['Load'].loc[hour]*result10.params[4] + \n",
    "                                                test_data_copy['DA_std'].loc[hour]*result10.params[3] + \n",
    "                                                test_data_copy['RT_std'].loc[hour]*result10.params[2] + \n",
    "                                                test_data_copy['LMP_DA'].loc[hour]*result10.params[1] + \n",
    "                                                result10.params[0])\n",
    "            except:\n",
    "                errors += 1\n",
    "                print('\\nerror occurred at i={} month={}'.format(i,j))\n",
    "                continue\n",
    "        PE_10_list.append(pe(test_data_copy['fitted RT (10)'], test_data_copy['LMP_RT']).mean())\n",
    "        \n",
    "    print(f'\\r{100*(i+1)/n}%',end='')\n",
    "\n",
    "PE_10_array = np.array(PE_10_list)\n",
    "\n",
    "print('\\nMAPE \\u00B1 std')\n",
    "print('10 predictors: {} \\u00B1 {}'.format(PE_10_array.mean(), PE_10_array.std()))\n",
    "print('# of errors: {}'.format(errors))\n",
    "print('R^2: {}'.format(np.nanmean(np.array(r2))))\n",
    "print('Adj R^2: {}'.format(np.nanmean(np.array(r2adj))))\n",
    "means_std_dict(slopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing loads at critical times\n",
    "\n",
    "DART2018_ESS = DART2018_2\n",
    "ESS_size = 7000\n",
    "months = []\n",
    "for month in range(1,13):\n",
    "    by_month = DART2018[DART2018['Month']==month]\n",
    "    by_month_ESS = DART2018_ESS[DART2018_ESS['Month']==month]\n",
    "\n",
    "    study = by_month.groupby('Local Time')[['LMP_RT','LMP_DA','Load']].mean()\n",
    "    \n",
    "    critical = crit_pts2(study)[0]\n",
    "    for t in critical:\n",
    "        by_month_ESS.loc[by_month_ESS['Local Time'] == t, 'Load'] -= ESS_size\n",
    "    \n",
    "    study2 = by_month_ESS.groupby('Local Time')[['LMP_RT','LMP_DA','Load']].mean()\n",
    "\n",
    "    months.append(by_month_ESS)\n",
    "    \n",
    "    plt.title('Month {}'.format(month))\n",
    "    plt.plot(study.index, study['LMP_RT'], 'orange', label='RT Means')\n",
    "    plt.plot(crit_pts2(study)[0],crit_pts2(study)[1],'ro')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(study.index, study['Load'], 'red')\n",
    "    plt.plot(study2.index, study2['Load'], 'blue', label='ESS Load')\n",
    "    plt.title('Month {}'.format(month))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "DART2018_ESS = pd.concat(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding previous-interval and moving averages on updated dataset (with reduced load)\n",
    "\n",
    "DART2018_ESS['Load_Diff'] = DART2018_ESS['Load'] - DART2018_ESS['DA Load']\n",
    "DART2018_ESS['Spread'] = DART2018_ESS['LMP_DA'] - DART2018_ESS['LMP_RT']\n",
    "DART2018_ESS = DART2018_ESS.rename(columns={' Wind':'Wind'})\n",
    "\n",
    "DART2018_ESS_1 = DART2018_ESS.iloc[1:,:]\n",
    "previous_load_diff = DART2018_ESS['Load_Diff'].iloc[0:-1].values\n",
    "DART2018_ESS_1['Previous_Load_Diff'] = previous_load_diff\n",
    "previous_RT = DART2018_ESS['LMP_RT'].iloc[0:-1].values\n",
    "DART2018_ESS_1['Previous_RT'] = previous_RT\n",
    "previous_spread = DART2018_ESS['Spread'].iloc[0:-1].values\n",
    "DART2018_ESS_1['Previous_Spread'] = previous_RT\n",
    "\n",
    "step = 3\n",
    "previous_RT = DART2018_ESS_1['LMP_RT'].iloc[0:(DART2018_ESS_1.shape[0] - step)].values\n",
    "previous_means_RT = np.empty(previous_RT.size)\n",
    "previous_load = DART2018_ESS_1['Load_Diff'].iloc[0:(DART2018_ESS_1.shape[0] - step)].values\n",
    "previous_means_load = np.empty(previous_load.size)\n",
    "for i in range(DART2018_ESS_1.shape[0] - step):\n",
    "    previous_means_RT[i] = previous_RT[i:i+step].mean()\n",
    "    previous_means_load[i] = previous_load[i:i+step].mean()\n",
    "DART2018_ESS_2 = DART2018_ESS_1.iloc[step:,:]\n",
    "DART2018_ESS_2['Previous_RT_2'] = previous_means_RT\n",
    "DART2018_ESS_2['Previous_Load_Diff_2'] = previous_means_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating price with peak-only MLR\n",
    "\n",
    "## Fixed a couple of things and te\n",
    "\n",
    "df = DART2018_2\n",
    "df2 = DART2018_ESS_2\n",
    "total_err = []\n",
    "\n",
    "r2 = []\n",
    "r2adj = []\n",
    "for i in range(10):\n",
    "    for j in range(1,13):\n",
    "        by_month = df[df['Month']==j]\n",
    "        study = by_month.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Load_Diff', 'Previous_Load_Diff',\n",
    "                                                  'Previous_RT', 'Previous_Load_Diff_2', 'Previous_RT_2']].mean()\n",
    "        study['RT_std'] = by_month.groupby('Local Time')['LMP_RT'].std()\n",
    "        study['DA_std'] = by_month.groupby('Local Time')['LMP_DA'].std()   \n",
    "\n",
    "        by_month2 = df2[df2['Month']==j]\n",
    "        study2 = by_month2.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Load_Diff', 'Previous_Load_Diff',\n",
    "                                                  'Previous_RT', 'Previous_Load_Diff_2', 'Previous_RT_2']].mean()\n",
    "        study2['RT_std'] = by_month2.groupby('Local Time')['LMP_RT'].std()\n",
    "        study2['DA_std'] = by_month2.groupby('Local Time')['LMP_DA'].std() \n",
    "        study2['ESS RT'] = study2['LMP_RT']\n",
    "\n",
    "        critical = crit_pts2(study)[0]\n",
    "        critical_df = study.loc[critical]\n",
    "        result_peak = smf.ols(formula=\"\"\"LMP_RT ~ LMP_DA + RT_std + DA_std + Load + Wind + Load_Diff + Previous_Load_Diff\n",
    "                                    + Previous_Load_Diff_2 + Previous_RT + Previous_RT_2\"\"\", data=critical_df).fit()\n",
    "        r2.append(result_peak.rsquared)\n",
    "        r2adj.append(result_peak.rsquared_adj)\n",
    "        for hour in study.index:\n",
    "            if hour in critical:\n",
    "                if False == True:\n",
    "                    study2['ESS RT'].loc[hour] = (\n",
    "                                            study2['Previous_RT_2'].loc[hour]*result_peak.params[10] +  \n",
    "                                            study2['Previous_RT'].loc[hour]*result_peak.params[9] +\n",
    "                                            study2['Previous_Load_Diff_2'].loc[hour]*result_peak.params[8] + \n",
    "                                            study2['Previous_Load_Diff'].loc[hour]*result_peak.params[7] + \n",
    "                                            study2['Load_Diff'].loc[hour]*result_peak.params[6] + \n",
    "                                            study2['Wind'].loc[hour]*result_peak.params[5] + \n",
    "                                            study2['Load'].loc[hour]*result_peak.params[4] + \n",
    "                                            study2['DA_std'].loc[hour]*result_peak.params[3] + \n",
    "                                            study2['RT_std'].loc[hour]*result_peak.params[2] + \n",
    "                                            study2['LMP_DA'].loc[hour]*result_peak.params[1] + \n",
    "                                            result_peak.params[0])\n",
    "                study2['ESS RT'].loc[hour] = (\n",
    "                                        study2['Previous_RT_2'].loc[hour]*slopes_10['Previous_RT_2'][0] +  \n",
    "                                        study2['Previous_RT'].loc[hour]*slopes_10['Previous_RT'][0] +\n",
    "                                        study2['Previous_Load_Diff_2'].loc[hour]*slopes_10['Previous_Load_Diff_2'][0] + \n",
    "                                        study2['Previous_Load_Diff'].loc[hour]*slopes_10['Previous_Load_Diff'][0] + \n",
    "                                        study2['Load_Diff'].loc[hour]*slopes_10['Load_Diff'][0] + \n",
    "                                        study2['Wind'].loc[hour]*slopes_10['Wind'][0] + \n",
    "                                        study2['Load'].loc[hour]*slopes_10['Load'][0] + \n",
    "                                        study2['DA_std'].loc[hour]*slopes_10['DA_std'][0] + \n",
    "                                        study2['RT_std'].loc[hour]*slopes_10['RT_std'][0] + \n",
    "                                        study2['LMP_DA'].loc[hour]*slopes_10['LMP_DA'][0] + \n",
    "                                        slopes_10['Intercept'][0])            \n",
    "                p_err = 100*(study2['ESS RT'].loc[hour] - study2['LMP_RT'].loc[hour])/study2['LMP_RT'].loc[hour]\n",
    "                total_err.append(p_err)\n",
    "        if i == 0:\n",
    "            plt.plot(study2.index, study2['LMP_RT'], 'blue', label='RT', alpha=0.5)\n",
    "            plt.plot(study2.index, study2['ESS RT'], 'red', label='ESS')\n",
    "            plt.title('Month {}'.format(j))\n",
    "            plt.ylabel('RT Means ($/MWh)')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "round(np.array(total_err).mean(),6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian optimization of ORT hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import Optimizer\n",
    "from skopt.space import Real, Categorical, Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan = DART2018[DART2018['Month']==1]\n",
    "dataset = jan.loc[:,['LMP_DA','Load','Wind','Previous_RT_MA','Hour','Day','LMP_RT','Minute of Day']]\n",
    "hours = []\n",
    "for time in dataset['Hour']:\n",
    "    hours.append(time.hour)\n",
    "dataset['Hour'] = np.array(hours)\n",
    "\n",
    "train_days = random.sample(range(1,32), 15)\n",
    "train = dataset.loc[dataset['Day'].isin(train_days)].dropna()\n",
    "test = dataset.loc[~dataset['Day'].isin(train_days)].dropna()\n",
    "\n",
    "X = train.iloc[:,0:5]\n",
    "y = train.iloc[:,6]\n",
    "\n",
    "X_test = test.iloc[:,0:5]\n",
    "y_test = test.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_loss(next_x):\n",
    "    xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = next_x[0], max_depth = next_x[1], \n",
    "                              learning_rate = next_x[2], alpha = next_x[3], colsample_bytree = next_x[4],\n",
    "                             min_child_weight = next_x[5])\n",
    "    xg_reg.fit(X, y)\n",
    "    y_xgb = xg_reg.predict(X_test)\n",
    "    return pe(y_xgb, y_test)[pe(y_xgb, y_test)!=np.inf].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(dimensions, start_points, times, function):\n",
    "\n",
    "    start_points, times = int(start_points), int(times)\n",
    "    # initial_point_generator, njobs not found\n",
    "    opt = Optimizer(list(dimensions.values()), base_estimator=\"gp\", n_initial_points=start_points,\n",
    "                    acq_func=\"gp_hedge\", acq_optimizer=\"auto\", random_state=0)\n",
    "\n",
    "    loss_vector = np.zeros(times)\n",
    "    params_list = []\n",
    "    for j in range(len(opt.ask())):\n",
    "        params_list.append(list())\n",
    "    best_loss = 1e10\n",
    "    for i in range(times):\n",
    "        next_x = opt.ask()\n",
    "        loss = function(next_x)\n",
    "        res = opt.tell(next_x, loss)\n",
    "        loss_vector[i] = loss\n",
    "        for p in range(len(next_x)):\n",
    "            params_list[p].append(next_x[p])\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_params = next_x\n",
    "        print(f\"\\rOptimization {100 * (i+1)/times}% completed. Best loss: {best_loss:.4}   \", end=\"\")\n",
    "    return best_loss, best_params, loss_vector, params_list\n",
    "\n",
    "dimensions = {'n_estimators': Integer(1, 20, prior='uniform'),\n",
    "              'max_depth': Integer(1, 5, prior='uniform'),\n",
    "              'learning_rate': Real(0.1, 0.9, prior='uniform'),\n",
    "              'alpha': Integer(5, 50, prior='uniform'),\n",
    "              'colsample_bytree': Real(0.2, 0.8, prior='uniform'),\n",
    "              'min_child_weight': Integer(1,100, prior='log-uniform')}\n",
    "\n",
    "loss, params, loss_vector, params_vector = optimize(dimensions, 1e5, 1e3, xgb_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DART2018.loc[:,['LMP_DA','Load','Wind','Previous_RT_MA','Hour','Day','LMP_RT','Minute of Day']]\n",
    "hours = []\n",
    "for time in dataset['Hour']:\n",
    "    hours.append(time.hour)\n",
    "dataset['Hour'] = np.array(hours)\n",
    "\n",
    "train, test = train_test_split(dataset, test_size = 0.5)\n",
    "\n",
    "train.index = train['Minute of Day']\n",
    "X = train.iloc[:,0:5]\n",
    "y = train.iloc[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.6,'learning_rate': 0.2,'max_depth': 5, 'alpha': 10}\n",
    "#params = {\"objective\":\"reg:linear\",'max_depth': 4}\n",
    "xg_reg = xgb.train(params=params, dtrain=data_dmatrix, num_boost_round=10)\n",
    "xgb.plot_tree(xg_reg,num_trees=0,rankdir='LR')\n",
    "plt.rcParams['figure.figsize'] = [50, 10]\n",
    "\n",
    "model = XGBRegressor(max_depth=4)\n",
    "model.fit(X, y)\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "plot_tree(model, ax=ax, rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
