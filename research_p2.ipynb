{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPP_path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)'\n",
    "\n",
    "def add_info(df):\n",
    "    intervals = df[df.columns[0]].values.tolist()\n",
    "    dates = []\n",
    "    times = []\n",
    "    weekday = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hour = []\n",
    "    minute_of_day = []\n",
    "    for interval in intervals:\n",
    "        date = interval.split(' ')[0]\n",
    "        try:\n",
    "            date = dt.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "        except:\n",
    "            date = dt.datetime.strptime(date,'%m/%d/%Y').date()            \n",
    "        dates.append(date)\n",
    "        months.append(date.month)\n",
    "        days.append(date.day)\n",
    "        if date.weekday() < 5:\n",
    "            weekday.append(True)\n",
    "        else:\n",
    "            weekday.append(False)\n",
    "        time = interval.split(' ')[1].split('.')[0]\n",
    "        time = dt.datetime.strptime(time,'%H:%M:%S').time()\n",
    "        times.append(time)\n",
    "        hour.append(dt.time(time.hour))\n",
    "        minute_of_day.append(60*time.hour+time.minute)\n",
    "    df['Local Date'] = np.array(dates)\n",
    "    df['Local Time'] = np.array(times)\n",
    "    df['Hour'] = np.array(hour)\n",
    "    df['Weekday'] = np.array(weekday)\n",
    "    df['Month'] = np.array(months)\n",
    "    df['Day'] = np.array(days)\n",
    "    df['Minute of Day'] = np.array(minute_of_day)\n",
    "    return df\n",
    "\n",
    "def GMT2CT(s):\n",
    "    date = s.split('T')[0]\n",
    "    date = dt.datetime.strptime(date,'%Y-%m-%d').date()\n",
    "    time = s.split('T')[1][:-1]\n",
    "    hour = int(time.split(':')[0])\n",
    "    if hour >= 6:\n",
    "        hour = hour - 6\n",
    "    else:\n",
    "        hour = 24 + (hour - 6)\n",
    "        date = date - timedelta(1)\n",
    "    time = str(hour) + ':' + time.split(':')[1] + ':' + time.split(':')[2]\n",
    "    time = dt.datetime.strptime(time,'%H:%M:%S').time()\n",
    "    return [date, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_mix_2018 = pd.read_csv(SPP_path + '\\Generation Mix By Fuel Type\\GenMix_2018.csv')\n",
    "list_intervals = gen_mix_2018[gen_mix_2018.columns[0]].values.tolist()\n",
    "local_time = []\n",
    "local_date = []\n",
    "for value in list_intervals:\n",
    "    local_date.append(GMT2CT(value)[0])\n",
    "    local_time.append(GMT2CT(value)[1])\n",
    "gen_mix_2018['Local Date'] = np.array(local_date)\n",
    "gen_mix_2018['Local Time'] = np.array(local_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_31 = []\n",
    "for n in range(1,10):\n",
    "    days_31.append('0'+str(n))\n",
    "for n in range(10,32):\n",
    "    days_31.append(str(n))\n",
    "cal_dict = {'01':days_31,\n",
    "            '02':days_31[0:28],\n",
    "            '03':days_31,\n",
    "           '04':days_31[0:-1],\n",
    "            '05':days_31,\n",
    "            '06':days_31[0:-1],\n",
    "           '07':days_31,\n",
    "            '08':days_31,\n",
    "            '09':days_31[0:-1],\n",
    "           '10':days_31,\n",
    "           '11':days_31[0:-1],\n",
    "           '12':days_31}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interval</th>\n",
       "      <th>GMT Interval</th>\n",
       "      <th>Settlement Location Name</th>\n",
       "      <th>PNODE Name</th>\n",
       "      <th>LMP</th>\n",
       "      <th>MLC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>MEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>01/01/2018 06:05:00</td>\n",
       "      <td>AEC</td>\n",
       "      <td>SOUC</td>\n",
       "      <td>255.8769</td>\n",
       "      <td>5.4654</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>250.4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>01/01/2018 06:05:00</td>\n",
       "      <td>AECC_CSWS</td>\n",
       "      <td>CSWS_AECC_LA</td>\n",
       "      <td>262.5802</td>\n",
       "      <td>5.0312</td>\n",
       "      <td>7.1375</td>\n",
       "      <td>250.4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>01/01/2018 06:05:00</td>\n",
       "      <td>AECC_ELKINS</td>\n",
       "      <td>CSWSELKINSUNELKINS_RA</td>\n",
       "      <td>270.2234</td>\n",
       "      <td>3.0020</td>\n",
       "      <td>16.8099</td>\n",
       "      <td>250.4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>01/01/2018 06:05:00</td>\n",
       "      <td>AECC_FITZHUGH</td>\n",
       "      <td>CSWSFITZHUGHPLT1</td>\n",
       "      <td>257.2724</td>\n",
       "      <td>6.8609</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>250.4115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>01/01/2018 06:05:00</td>\n",
       "      <td>AECC_FLTCREEK</td>\n",
       "      <td>CSWSFLINTCRKUN1_JOU_AECC_RA</td>\n",
       "      <td>266.5657</td>\n",
       "      <td>-3.0577</td>\n",
       "      <td>19.2119</td>\n",
       "      <td>250.4115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Interval         GMT Interval  Settlement Location Name  \\\n",
       "0  01/01/2018 00:05:00  01/01/2018 06:05:00                       AEC   \n",
       "1  01/01/2018 00:05:00  01/01/2018 06:05:00                 AECC_CSWS   \n",
       "2  01/01/2018 00:05:00  01/01/2018 06:05:00               AECC_ELKINS   \n",
       "3  01/01/2018 00:05:00  01/01/2018 06:05:00             AECC_FITZHUGH   \n",
       "4  01/01/2018 00:05:00  01/01/2018 06:05:00             AECC_FLTCREEK   \n",
       "\n",
       "                    PNODE Name       LMP     MLC      MCC       MEC  \n",
       "0                         SOUC  255.8769  5.4654   0.0000  250.4115  \n",
       "1                 CSWS_AECC_LA  262.5802  5.0312   7.1375  250.4115  \n",
       "2        CSWSELKINSUNELKINS_RA  270.2234  3.0020  16.8099  250.4115  \n",
       "3             CSWSFITZHUGHPLT1  257.2724  6.8609   0.0000  250.4115  \n",
       "4  CSWSFLINTCRKUN1_JOU_AECC_RA  266.5657 -3.0577  19.2119  250.4115  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening and concatenating RT datasets\n",
    "path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)\\RT\\2018'\n",
    "RT_path = 'RTBM-LMP-DAILY-SL-2018'\n",
    "end = '.csv'\n",
    "dfs = []\n",
    "for key in cal_dict.keys():\n",
    "    for value in cal_dict[key]:\n",
    "        dfs.append(pd.read_csv(path+'\\\\'+key+'\\\\By_Day\\\\'+RT_path+key+value+end))\n",
    "RT = pd.concat(dfs)\n",
    "RT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interval</th>\n",
       "      <th>LMP</th>\n",
       "      <th>Local Date</th>\n",
       "      <th>Local Time</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Minute of Day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interval</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:05:00</th>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>246.567610</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:10:00</th>\n",
       "      <td>01/01/2018 00:10:00</td>\n",
       "      <td>64.864068</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:15:00</th>\n",
       "      <td>01/01/2018 00:15:00</td>\n",
       "      <td>164.686048</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:20:00</th>\n",
       "      <td>01/01/2018 00:20:00</td>\n",
       "      <td>173.867912</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:25:00</th>\n",
       "      <td>01/01/2018 00:25:00</td>\n",
       "      <td>163.353003</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Interval         LMP  Local Date Local Time  \\\n",
       "Interval                                                                      \n",
       "01/01/2018 00:05:00  01/01/2018 00:05:00  246.567610  2018-01-01   00:05:00   \n",
       "01/01/2018 00:10:00  01/01/2018 00:10:00   64.864068  2018-01-01   00:10:00   \n",
       "01/01/2018 00:15:00  01/01/2018 00:15:00  164.686048  2018-01-01   00:15:00   \n",
       "01/01/2018 00:20:00  01/01/2018 00:20:00  173.867912  2018-01-01   00:20:00   \n",
       "01/01/2018 00:25:00  01/01/2018 00:25:00  163.353003  2018-01-01   00:25:00   \n",
       "\n",
       "                         Hour  Weekday  Month  Day  Minute of Day  \n",
       "Interval                                                           \n",
       "01/01/2018 00:05:00  00:00:00     True      1    1              5  \n",
       "01/01/2018 00:10:00  00:00:00     True      1    1             10  \n",
       "01/01/2018 00:15:00  00:00:00     True      1    1             15  \n",
       "01/01/2018 00:20:00  00:00:00     True      1    1             20  \n",
       "01/01/2018 00:25:00  00:00:00     True      1    1             25  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RT2018_aggloc = RT.groupby('Interval')[['Interval',' LMP']].agg({'Interval':'first',\n",
    "                                                                    ' LMP':'mean'})\n",
    "RT2018 = add_info(RT2018_aggloc)\n",
    "RT2018 = RT2018.rename(columns={' LMP':'LMP'})\n",
    "RT2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interval</th>\n",
       "      <th>GMTIntervalEnd</th>\n",
       "      <th>Settlement Location</th>\n",
       "      <th>Pnode</th>\n",
       "      <th>LMP</th>\n",
       "      <th>MLC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>MEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AEC</td>\n",
       "      <td>SOUC</td>\n",
       "      <td>39.6809</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>38.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_CSWS</td>\n",
       "      <td>CSWS_AECC_LA</td>\n",
       "      <td>38.3723</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>-0.5781</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_ELKINS</td>\n",
       "      <td>CSWSELKINSUNELKINS_RA</td>\n",
       "      <td>38.4189</td>\n",
       "      <td>0.4093</td>\n",
       "      <td>-0.2077</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_FITZHUGH</td>\n",
       "      <td>CSWSFITZHUGHPLT1</td>\n",
       "      <td>38.5076</td>\n",
       "      <td>1.0540</td>\n",
       "      <td>-0.7637</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_FLTCREEK</td>\n",
       "      <td>CSWSFLINTCRKUN1_JOU_AECC_RA</td>\n",
       "      <td>37.8395</td>\n",
       "      <td>-0.4387</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Interval       GMTIntervalEnd Settlement Location  \\\n",
       "0  01/01/2018 01:00:00  01/01/2018 07:00:00                 AEC   \n",
       "1  01/01/2018 01:00:00  01/01/2018 07:00:00           AECC_CSWS   \n",
       "2  01/01/2018 01:00:00  01/01/2018 07:00:00         AECC_ELKINS   \n",
       "3  01/01/2018 01:00:00  01/01/2018 07:00:00       AECC_FITZHUGH   \n",
       "4  01/01/2018 01:00:00  01/01/2018 07:00:00       AECC_FLTCREEK   \n",
       "\n",
       "                         Pnode      LMP     MLC     MCC      MEC  \n",
       "0                         SOUC  39.6809  0.8665  0.5970  38.2174  \n",
       "1                 CSWS_AECC_LA  38.3723  0.7331 -0.5781  38.2173  \n",
       "2        CSWSELKINSUNELKINS_RA  38.4189  0.4093 -0.2077  38.2173  \n",
       "3             CSWSFITZHUGHPLT1  38.5076  1.0540 -0.7637  38.2173  \n",
       "4  CSWSFLINTCRKUN1_JOU_AECC_RA  37.8395 -0.4387  0.0609  38.2173  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening and concatenating DA datasets\n",
    "path = r'C:\\Users\\felip\\Desktop\\Electricity\\Energy Market\\Energy Market (SPP)\\DA\\2018'\n",
    "DA_path = 'DA-LMP-SL-2018'\n",
    "end = '0100.csv'\n",
    "dfs = []\n",
    "for key in cal_dict.keys():\n",
    "    for value in cal_dict[key]:\n",
    "        dfs.append(pd.read_csv(path+'\\\\'+key+'\\\\By_Day\\\\'+DA_path+key+value+end))\n",
    "DA2018 = pd.concat(dfs)\n",
    "DA2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Interval</th>\n",
       "      <th>GMTIntervalEnd</th>\n",
       "      <th>Settlement Location</th>\n",
       "      <th>Pnode</th>\n",
       "      <th>LMP</th>\n",
       "      <th>MLC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>MEC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AEC</td>\n",
       "      <td>SOUC</td>\n",
       "      <td>39.6809</td>\n",
       "      <td>0.8665</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>38.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_CSWS</td>\n",
       "      <td>CSWS_AECC_LA</td>\n",
       "      <td>38.3723</td>\n",
       "      <td>0.7331</td>\n",
       "      <td>-0.5781</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_ELKINS</td>\n",
       "      <td>CSWSELKINSUNELKINS_RA</td>\n",
       "      <td>38.4189</td>\n",
       "      <td>0.4093</td>\n",
       "      <td>-0.2077</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_FITZHUGH</td>\n",
       "      <td>CSWSFITZHUGHPLT1</td>\n",
       "      <td>38.5076</td>\n",
       "      <td>1.0540</td>\n",
       "      <td>-0.7637</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/01/2018 01:00:00</td>\n",
       "      <td>01/01/2018 07:00:00</td>\n",
       "      <td>AECC_FLTCREEK</td>\n",
       "      <td>CSWSFLINTCRKUN1_JOU_AECC_RA</td>\n",
       "      <td>37.8395</td>\n",
       "      <td>-0.4387</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>38.2173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Interval       GMTIntervalEnd Settlement Location  \\\n",
       "0  01/01/2018 01:00:00  01/01/2018 07:00:00                 AEC   \n",
       "1  01/01/2018 01:00:00  01/01/2018 07:00:00           AECC_CSWS   \n",
       "2  01/01/2018 01:00:00  01/01/2018 07:00:00         AECC_ELKINS   \n",
       "3  01/01/2018 01:00:00  01/01/2018 07:00:00       AECC_FITZHUGH   \n",
       "4  01/01/2018 01:00:00  01/01/2018 07:00:00       AECC_FLTCREEK   \n",
       "\n",
       "                         Pnode      LMP     MLC     MCC      MEC  \n",
       "0                         SOUC  39.6809  0.8665  0.5970  38.2174  \n",
       "1                 CSWS_AECC_LA  38.3723  0.7331 -0.5781  38.2173  \n",
       "2        CSWSELKINSUNELKINS_RA  38.4189  0.4093 -0.2077  38.2173  \n",
       "3             CSWSFITZHUGHPLT1  38.5076  1.0540 -0.7637  38.2173  \n",
       "4  CSWSFLINTCRKUN1_JOU_AECC_RA  37.8395 -0.4387  0.0609  38.2173  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DA2018_aggloc = DA2018.groupby('Interval')[['Interval','LMP']].agg({'Interval':'first',\n",
    "                                                                    'LMP':'mean'})\n",
    "DA2018_aggloc = add_info(DA2018_aggloc)\n",
    "DA2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr(prediction, test):\n",
    "    return ((prediction - test)**2).sum()\n",
    "def pe(prediction, test):\n",
    "    return (abs((prediction - test)/test))*100\n",
    "def filler(df, DA):\n",
    "    price_list = df[DA].values.tolist()\n",
    "    value = 0.0\n",
    "    new_list = []\n",
    "    for price in price_list:\n",
    "        if math.isnan(price)==True:\n",
    "            new_list.append(value)\n",
    "        else:\n",
    "            value = price\n",
    "            new_list.append(price)\n",
    "    df[DA] = np.array(new_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = DA2018_aggloc.iloc[:,0:2].join(RT2018,how='right',lsuffix='_DA',rsuffix='_RT')\n",
    "DART2018_5min = filler(comparison, 'LMP_DA')\n",
    "gen_2018 = gen_mix_2018\n",
    "new_index = []\n",
    "for i in range(gen_2018.shape[0]):\n",
    "    new_index.append(gen_2018['Local Date'].iloc[i].strftime(\"%m/%d/%Y\") + ' ' + gen_2018['Local Time'].iloc[i].strftime(\"%H:%M:%S\"))\n",
    "gen_2018.index = np.array(new_index)\n",
    "DART_gen_2018 = gen_2018.join(DART2018_5min,how='right',lsuffix='_gen',rsuffix='_price')\n",
    "DART_gen_2018 = DART_gen_2018.rename(columns={' Average Actual Load':'Load',\n",
    "                                             ' Wind Self':'Wind',\n",
    "                                             ' Coal Market':'Coal_Mkt',\n",
    "                                             ' Coal Self':'Coal_Self',\n",
    "                                             'Local Time_price':'Local Time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssr_split4(df, s):\n",
    "    if df.shape[0]>(288/4):\n",
    "        smallest_y = 0\n",
    "        smallest_x = 0\n",
    "        for i in range(0, df.shape[0]):\n",
    "            try:\n",
    "                y_avg1 = df[s][0:i+1].values.mean()\n",
    "                y_avg2 = df[s][i:df.shape[0]].values.mean()\n",
    "                ssr = ((df[s][0:i+1].values - y_avg1)**2).sum() + ((df[s][i:df.shape[0]].values - y_avg2)**2).sum()\n",
    "                if i == 0:\n",
    "                    smallest_y = ssr\n",
    "                if ssr < smallest_y:\n",
    "                    smallest_y = ssr\n",
    "                    smallest_x = i\n",
    "            except:\n",
    "                y_avg1 = df[s].values.mean()\n",
    "                ssr = ((df[s][0:i+1].values - y_avg1)**2).sum()\n",
    "                if ssr < smallest_y:\n",
    "                    smallest_y = ssr\n",
    "                    smallest_x = i\n",
    "        smallest = df.iloc[0:smallest_x+1]\n",
    "        largest = df.iloc[smallest_x+1:]\n",
    "        if smallest.shape[0] > largest.shape[0]:\n",
    "            temp = smallest\n",
    "            smallest = largest\n",
    "            largest = temp\n",
    "        concat = [smallest] + [ssr_split4(largest, s)]\n",
    "        return concat\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "def unnest(l, empty_l):\n",
    "    for i in l:\n",
    "        if type(i) == list:\n",
    "            unnest(i, empty_l)\n",
    "        else:\n",
    "            empty_l.append(i)\n",
    "            \n",
    "def unnested_df(l):\n",
    "    for df in l:\n",
    "        means = np.empty(df.shape[0])\n",
    "        means.fill(df['LMP_RT'].mean())\n",
    "        df['Means'] = means\n",
    "    unnested = pd.concat(l)\n",
    "    unnested['Time'] = unnested.index\n",
    "    unnested = unnested.sort_values('Time')\n",
    "    return unnested\n",
    "\n",
    "def sklearn_clusters(train_df, test_df):\n",
    "    # converting datetime to float\n",
    "    minutes = []\n",
    "    for time in train_df.index:\n",
    "        minutes.append(time.hour * 60 + time.minute)\n",
    "    train_df['minutes'] = np.array(minutes)\n",
    "    minutes = []\n",
    "    for time in test_df.index:\n",
    "        minutes.append(time.hour * 60 + time.minute)\n",
    "    test_df['minutes'] = np.array(minutes)\n",
    "    # testing existing module\n",
    "    X = train_df['minutes'].values.reshape(-1,1)\n",
    "    y = train_df['LMP_RT']\n",
    "    # Fit regression model\n",
    "    regr_1 = DecisionTreeRegressor(max_depth=3)\n",
    "    regr_1.fit(X, y)\n",
    "    # Predict\n",
    "    X_test = test_df['minutes'].values.reshape(-1,1)\n",
    "    y_1 = regr_1.predict(X_test)\n",
    "    test_df['sklearn cluster'] = y_1\n",
    "    price_leaves = []\n",
    "    for price in y_1:\n",
    "        if price not in price_leaves:\n",
    "            price_leaves.append(price)\n",
    "    branch_dfs = []\n",
    "    for price in price_leaves:\n",
    "        branch_dfs.append(test_df[test_df['sklearn cluster']==price])\n",
    "    return branch_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "0%\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "10%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "20%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "30%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "40%\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "50%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "60%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "70%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "80%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "90%\n",
      "custom cluster:\n",
      "\tMAPE: 14.419813992680584;\n",
      "\tMean R^2: 0.8826842744925303;\n",
      "sklearn clusters:\n",
      "\tMAPE: 7.541004674056056;\n",
      "\tMean R^2: nan;\n",
      "errors filling predictions: 0\n",
      "mean of all valid R^2: -inf\n",
      "2.7310924369747913% of R^2 were invalid (nan or inf)\n"
     ]
    }
   ],
   "source": [
    "# system-wide, 5 predictors\n",
    "# multi or single clustered -- comparings R^2\n",
    "\n",
    "np.seterr(divide='print')\n",
    "def err_handler(type, flag):\n",
    "    print(\"Floating point error (%s) with flag %s\" % (type, flag))\n",
    "saved_handler = np.seterrcall(err_handler)\n",
    "save_err = np.seterr(all='call')\n",
    "\n",
    "n = 10\n",
    "custom_PE_array = np.empty(n)\n",
    "custom_R2_array = np.empty(n)\n",
    "sklearn_PE_array = np.empty(n)\n",
    "sklearn_R2_array = np.empty(n)\n",
    "\n",
    "check = 0\n",
    "R2 = []\n",
    "\n",
    "for i in range(n):\n",
    "    custom_PE_list = []\n",
    "    custom_R2_list = []\n",
    "    sklearn_PE_list = []\n",
    "    sklearn_R2_list = []\n",
    "\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "\n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "\n",
    "        test_data_copy = test_data.copy()\n",
    "        \n",
    "        size = test_data_copy.index.shape[0]\n",
    "        test_data_copy['fitted RT (custom)'] = np.zeros(size)            \n",
    "        cl = []\n",
    "        unnest(ssr_split4(train_data, 'LMP_RT'), cl)\n",
    "        for cluster in cl:\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            custom_R2_list.append(result.rsquared)\n",
    "            for hour in cluster.index:\n",
    "                try:\n",
    "                    test_data_copy['fitted RT (custom)'].loc[hour] = ( \n",
    "                                                test_data_copy['Wind'].loc[hour]*result.params[5] + \n",
    "                                                test_data_copy['Load'].loc[hour]*result.params[4] + \n",
    "                                                test_data_copy['DA_sem'].loc[hour]*result.params[3] + \n",
    "                                                test_data_copy['RT_sem'].loc[hour]*result.params[2] + \n",
    "                                                test_data_copy['LMP_DA'].loc[hour]*result.params[1] + \n",
    "                                                result.params[0])\n",
    "                except:\n",
    "                    continue\n",
    "        custom_PE_list.append(pe(test_data_copy['fitted RT (custom)'], test_data_copy['LMP_RT']).mean())\n",
    "        \n",
    "                \n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        size = test_data_copy.index.shape[0]\n",
    "        test_data_copy['fitted RT (sklearn)'] = np.zeros(size) \n",
    "        for cluster in branch_dfs:\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            sklearn_R2_list.append(result.rsquared)\n",
    "            R2.append(result.rsquared)\n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (sklearn)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result.params[1] + \n",
    "                                            result.params[0])\n",
    "        sklearn_PE_list.append(pe(test_data_copy['fitted RT (sklearn)'], test_data_copy['LMP_RT']).mean())\n",
    "            \n",
    "            \n",
    "        check += np.where(test_data_copy['fitted RT (custom)']==0.0,1,0).sum()\n",
    "        check += np.where(test_data_copy['fitted RT (sklearn)']==0.0,1,0).sum()\n",
    "        if check > 0:\n",
    "            print(check)\n",
    "            break\n",
    "                \n",
    "    print('{}%'.format(10*i))\n",
    "\n",
    "    custom_PE_array[i] = np.array(custom_PE_list).mean()\n",
    "    custom_R2_array[i] = np.array(custom_R2_list).mean()\n",
    "    sklearn_PE_array[i] = np.array(sklearn_PE_list).mean()\n",
    "    sklearn_R2_array[i] = np.array(sklearn_R2_list).mean()\n",
    "\n",
    "\n",
    "print('custom cluster:\\n\\tMAPE: {};\\n\\tMean R^2: {};'.format(custom_PE_array.mean(),custom_R2_array.mean()))\n",
    "print('sklearn clusters:\\n\\tMAPE: {};\\n\\tMean R^2: {};'.format(sklearn_PE_array.mean(),sklearn_R2_array.mean()))\n",
    "print('errors filling predictions: {}'.format(check))\n",
    "len1 = len(R2)\n",
    "for value in R2:\n",
    "    if (value == np.inf) or (value == -np.inf):\n",
    "        R2.remove(value)\n",
    "len2 = len(R2)\n",
    "print('mean of all valid R^2: {}'.format(np.nanmean(R2)))\n",
    "print('{}% of R^2 were invalid (nan or inf)'.format(100*(1 - len2/len1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raising so many warnings, but was not able to track them down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means_dict(d):\n",
    "    means_d = {}\n",
    "    for key in d.keys():\n",
    "        means_d[key] = np.nanmean(np.array(d[key]))\n",
    "    return means_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "0%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "10%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "20%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "30%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "40%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "50%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "60%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "70%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "80%\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (invalid value) with flag 8\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "Floating point error (divide by zero) with flag 1\n",
      "90%\n",
      "MAPE: 7.468331028253023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.3991195756369897,\n",
       " 'LMP_DA': 0.3840053734078047,\n",
       " 'RT_sem': 0.09733417842196931,\n",
       " 'DA_sem': 0.3704615162996058,\n",
       " 'Load': 0.23155805749534794,\n",
       " 'Wind': 0.2079529098505968}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(divide='print')\n",
    "def err_handler(type, flag):\n",
    "    print(\"Floating point error (%s) with flag %s\" % (type, flag))\n",
    "saved_handler = np.seterrcall(err_handler)\n",
    "save_err = np.seterr(all='call')\n",
    "n = 10\n",
    "sklearn_PE_array = np.empty(n)\n",
    "pvalues = {'Intercept':[],\n",
    "          'LMP_DA':[],\n",
    "          'RT_sem':[],\n",
    "          'DA_sem':[],\n",
    "          'Load':[],\n",
    "          'Wind':[]}\n",
    "for i in range(n):\n",
    "    sklearn_PE_list = []\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "\n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['fitted RT (sklearn)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        for cluster in branch_dfs:\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (sklearn)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result.params[1] + \n",
    "                                            result.params[0])\n",
    "            for predictor in result.pvalues.index:\n",
    "                pvalues[predictor].append(result.pvalues.loc[predictor])\n",
    "        sklearn_PE_list.append(pe(test_data_copy['fitted RT (sklearn)'], test_data_copy['LMP_RT']).mean())\n",
    "    sklearn_PE_array[i] = np.array(sklearn_PE_list).mean()\n",
    "    print('{}%'.format(10*i))\n",
    "print('MAPE: {}'.format(sklearn_PE_array.mean()))\n",
    "means_dict(pvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions\n",
    "- Why does the warning come up when result is accessed?\n",
    "- Why are p-values so high? -> evaluate each regression separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "p-values:\n",
      "\tRT_sem: 0.09433255480519437\n",
      "\tDA_sem: 0.4068593140177411\n",
      "\tLMP_DA: 0.22959562937090458\n",
      "\tLoad: 0.2647393732592192\n",
      "\tWind: 0.24389890203040004\n"
     ]
    }
   ],
   "source": [
    "# calculate p-values for each predictor - forward selection\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore') # ignore errors for n=100\n",
    "n = 100\n",
    "RTsem_model = []\n",
    "DA_model = []\n",
    "load_model = []\n",
    "DAsem_model = []\n",
    "wind_model = []\n",
    "for i in range(n):\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "\n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['fitted RT (sklearn)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        for cluster in branch_dfs:\n",
    "            result = smf.ols(formula=\"LMP_RT ~ RT_sem\", data=cluster).fit()\n",
    "            RTsem_model.append(result.pvalues[1])\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA\", data=cluster).fit()\n",
    "            DA_model.append(result.pvalues[1])\n",
    "            result = smf.ols(formula=\"LMP_RT ~ Load\", data=cluster).fit()\n",
    "            load_model.append(result.pvalues[1])\n",
    "            result = smf.ols(formula=\"LMP_RT ~ DA_sem\", data=cluster).fit()\n",
    "            DAsem_model.append(result.pvalues[1])\n",
    "            result = smf.ols(formula=\"LMP_RT ~ Wind\", data=cluster).fit()\n",
    "            wind_model.append(result.pvalues[1])\n",
    "\n",
    "    if i%(10)==0:\n",
    "        print('{}%'.format(i))\n",
    "            \n",
    "print('p-values:')\n",
    "print('\\tRT_sem: {}'.format(np.nanmean(RTsem_model).mean()))\n",
    "print('\\tDA_sem: {}'.format(np.nanmean(DAsem_model).mean()))\n",
    "print('\\tLMP_DA: {}'.format(np.nanmean(DA_model).mean()))\n",
    "print('\\tLoad: {}'.format(np.nanmean(load_model).mean()))\n",
    "print('\\tWind: {}'.format(np.nanmean(wind_model).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020802914726145597"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.pvalues[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p-values test\n",
    "still many warnings, but results were (for n = 100 * 12):\n",
    "\n",
    "|factors|    factor p   | intercept p +- sem  |\n",
    "|-------|---------------|---------------------|\n",
    "|RT_sem |     0.094     |     0.011 +- 0.001  |\n",
    "|DA_sem |     0.407     |     0.061 +- 0.002  |\n",
    "|LMP_DA |     0.230     |     0.368 +- 0.003  |\n",
    "|Load   |     0.265     |     0.356 +- 0.003  |\n",
    "|Wind   |     0.244     |     0.073 +- 0.002  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "MAPE: 7.246475232222092\n",
      "MAPE for Adjusted R^2 > 0.8: 6.909833219612219\n",
      "48.333333333333336% of months had Adjusted R^2 > 0.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1311df489e8>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I'll ignore warnings for the following test\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "n = 10\n",
    "sklearn_PE_array = np.zeros(n)\n",
    "best_PE_array = np.zeros(n)\n",
    "all_R2adj = []\n",
    "all_PE = []\n",
    "k = 0\n",
    "r = 0.8\n",
    "for i in range(n):\n",
    "    sklearn_PE_list = []\n",
    "    best_PE_list = []\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "\n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['fitted RT (sklearn)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        month_R2adj = []\n",
    "        for cluster in branch_dfs:\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            month_R2adj.append(result.rsquared_adj)\n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (sklearn)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result.params[1] + \n",
    "                                            result.params[0])\n",
    "        \n",
    "        PE = pe(test_data_copy['fitted RT (sklearn)'], test_data_copy['LMP_RT']).mean()\n",
    "        \n",
    "        if np.nanmean(np.array(month_R2adj)) > r:\n",
    "            best_PE_list.append(PE)\n",
    "            k += 1\n",
    "        sklearn_PE_list.append(PE)\n",
    "        \n",
    "        all_R2adj.append(np.nanmean(np.array(month_R2adj)))\n",
    "        all_PE.append(PE)\n",
    "        \n",
    "    sklearn_PE_array[i] = np.array(sklearn_PE_list).mean()\n",
    "    best_PE_array[i] = np.array(best_PE_list).mean()\n",
    "\n",
    "    print('{}%'.format(10*i))\n",
    "print('MAPE: {}'.format(sklearn_PE_array.mean()))\n",
    "print('MAPE for Adjusted R^2 > {}: {}'.format(r, best_PE_array.mean()))\n",
    "print('{}% of months had Adjusted R^2 > {}'.format(100*k/(12*n), r))\n",
    "plt.plot(np.array(all_R2adj), np.array(all_PE), 'ro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "RT_sem + DA_sem + Wind's MAPE: 9.038660714968767\n",
      "all 5 predictors' MAPE: 7.316075494937268\n",
      "LMP_DA + Load + Wind's MAPE: 20.288377609050364\n"
     ]
    }
   ],
   "source": [
    "# compare MAPE between predictions with 3 vs 5 variables\n",
    "\n",
    "# I'll ignore warnings for the following test\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "n = 10\n",
    "PE_5_array = np.zeros(n)\n",
    "PE_3_array = np.zeros(n)\n",
    "PE_0_array = np.zeros(n)\n",
    "\n",
    "for i in range(n):\n",
    "    five_list = []\n",
    "    three_list = []\n",
    "    no_errors_list = []\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        \n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['fitted RT (all 5)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        test_data_copy['fitted RT (3)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        test_data_copy['fitted RT (no errors)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        for cluster in branch_dfs:\n",
    "            result5 = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            result3 = smf.ols(formula=\"LMP_RT ~ RT_sem + DA_sem + Wind\", data=cluster).fit()\n",
    "            result = smf.ols(formula=\"LMP_RT ~ LMP_DA + Load + Wind\", data=cluster).fit()            \n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (all 5)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result5.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result5.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result5.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result5.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result5.params[1] + \n",
    "                                            result5.params[0])\n",
    "                test_data_copy['fitted RT (3)'].loc[hour] = (  \n",
    "                                            test_data_copy['Wind'].loc[hour]*result3.params[3] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result3.params[2] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result3.params[1] + \n",
    "                                            result3.params[0])\n",
    "                test_data_copy['fitted RT (no errors)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result.params[3] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result.params[1] + \n",
    "                                            result.params[0])\n",
    "                \n",
    "        five_list.append(pe(test_data_copy['fitted RT (all 5)'], test_data_copy['LMP_RT']).mean())\n",
    "        three_list.append(pe(test_data_copy['fitted RT (3)'], test_data_copy['LMP_RT']).mean())\n",
    "        no_errors_list.append(pe(test_data_copy['fitted RT (no errors)'], test_data_copy['LMP_RT']).mean())\n",
    "\n",
    "        \n",
    "    PE_5_array[i] = np.array(five_list).mean()\n",
    "    PE_3_array[i] = np.array(three_list).mean()\n",
    "    PE_0_array[i] = np.array(no_errors_list).mean()\n",
    "    print('{}%'.format(10*i))\n",
    "    \n",
    "print('RT_sem + DA_sem + Wind\\'s MAPE: {}'.format(PE_3_array.mean()))\n",
    "print('all 5 predictors\\' MAPE: {}'.format(PE_5_array.mean()))\n",
    "print('LMP_DA + Load + Wind\\'s MAPE: {}'.format(PE_0_array.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RT_sem, DA_sem and Wind have significantly lower p-values, adding Load and LMP_DA to the model reduces MAPE.\n",
    "Simply using the most intuitive LMP_DA, Wind and Load yields poor predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n",
      "40%\n",
      "50%\n",
      "60%\n",
      "70%\n",
      "80%\n",
      "90%\n",
      "6 predictors' MAPE: 7.0467783130900585\n",
      "5 predictors' MAPE: 7.193938792486977\n"
     ]
    }
   ],
   "source": [
    "# let's add weekend/weekday feature to model \n",
    "# -- because of current structure, model will take mean of weekdays (1) and weekends (0)\n",
    "\n",
    "# compare MAPE between predictions with 6 vs 5 variables\n",
    "\n",
    "# I'll ignore warnings for the following test\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "n = 10\n",
    "PE_5_array = np.zeros(n)\n",
    "PE_6_array = np.zeros(n)\n",
    "weekday_p = []\n",
    "for i in range(n):\n",
    "    five_list = []\n",
    "    six_list = []\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Weekday']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Weekday']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        \n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['fitted RT (5)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        test_data_copy['fitted RT (6)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        for cluster in branch_dfs:\n",
    "            result5 = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind\", data=cluster).fit()\n",
    "            result6 = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind + Weekday\", data=cluster).fit()\n",
    "            weekday_p.append(result6.pvalues[-1])\n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (5)'].loc[hour] = ( \n",
    "                                            test_data_copy['Wind'].loc[hour]*result5.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result5.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result5.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result5.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result5.params[1] + \n",
    "                                            result5.params[0])\n",
    "                test_data_copy['fitted RT (6)'].loc[hour] = (  \n",
    "                                            test_data_copy['Weekday'].loc[hour]*result6.params[6] + \n",
    "                                            test_data_copy['Wind'].loc[hour]*result6.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result6.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result6.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result6.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result6.params[1] + \n",
    "                                            result6.params[0])\n",
    "                \n",
    "        five_list.append(pe(test_data_copy['fitted RT (5)'], test_data_copy['LMP_RT']).mean())\n",
    "        six_list.append(pe(test_data_copy['fitted RT (6)'], test_data_copy['LMP_RT']).mean())\n",
    "\n",
    "        \n",
    "    PE_5_array[i] = np.array(five_list).mean()\n",
    "    PE_6_array[i] = np.array(six_list).mean()\n",
    "    print('{}%'.format(10*i))\n",
    "    \n",
    "print('6 predictors\\' MAPE: {}'.format(PE_6_array.mean()))\n",
    "print('5 predictors\\' MAPE: {}'.format(PE_5_array.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding day of the week feature slightly improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "10%\n",
      "20%\n",
      "30%\n"
     ]
    }
   ],
   "source": [
    "# let's compare single-cluster vs multi-cluster\n",
    "\n",
    "# let's add weekend/weekday feature to model \n",
    "# -- because of current structure, model will take mean of weekdays (1) and weekends (0)\n",
    "\n",
    "# compare MAPE between predictions with 6 vs 5 variables\n",
    "\n",
    "# I'll ignore warnings for the following test\n",
    "\n",
    "np.seterr(divide='ignore',invalid='ignore')\n",
    "n = 10\n",
    "single_array = np.zeros(n)\n",
    "PE_6_array = np.zeros(n)\n",
    "weekday_p = []\n",
    "for i in range(n):\n",
    "    single_list = []\n",
    "    six_list = []\n",
    "    for j in range(1,13):\n",
    "        by_month = DART_gen_2018[DART_gen_2018['Month']==j]\n",
    "\n",
    "        train, test = train_test_split(by_month, test_size=0.33)\n",
    "\n",
    "        train_data = train.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Weekday']].mean()\n",
    "        train_data['RT_sem'] = train.groupby('Local Time')['LMP_RT'].sem()\n",
    "        train_data['DA_sem'] = train.groupby('Local Time')['LMP_DA'].sem()        \n",
    "        test_data = test.groupby('Local Time')[['LMP_RT','LMP_DA','Load','Wind','Weekday']].mean()\n",
    "        test_data['RT_sem'] = test.groupby('Local Time')['LMP_RT'].sem()\n",
    "        test_data['DA_sem'] = test.groupby('Local Time')['LMP_DA'].sem() \n",
    "        \n",
    "        test_data_copy = test_data.copy()\n",
    "\n",
    "        result = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind + Weekday\", data=train_data).fit()\n",
    "        test_data_copy['fitted RT (single)'] = (  \n",
    "                                            test_data_copy['Weekday']*result.params[6] + \n",
    "                                            test_data_copy['Wind']*result.params[5] + \n",
    "                                            test_data_copy['Load']*result.params[4] + \n",
    "                                            test_data_copy['DA_sem']*result.params[3] + \n",
    "                                            test_data_copy['RT_sem']*result.params[2] + \n",
    "                                            test_data_copy['LMP_DA']*result.params[1] + \n",
    "                                            result.params[0])\n",
    "        test_data_copy['fitted RT (6)'] = np.zeros(test_data_copy.index.shape[0])\n",
    "        branch_dfs = sklearn_clusters(train_data, test_data)\n",
    "        for cluster in branch_dfs:\n",
    "            result6 = smf.ols(formula=\"LMP_RT ~ LMP_DA + RT_sem + DA_sem + Load + Wind + Weekday\", data=cluster).fit()\n",
    "            weekday_p.append(result6.pvalues[-1])\n",
    "            for hour in cluster.index:\n",
    "                test_data_copy['fitted RT (6)'].loc[hour] = (  \n",
    "                                            test_data_copy['Weekday'].loc[hour]*result6.params[6] + \n",
    "                                            test_data_copy['Wind'].loc[hour]*result6.params[5] + \n",
    "                                            test_data_copy['Load'].loc[hour]*result6.params[4] + \n",
    "                                            test_data_copy['DA_sem'].loc[hour]*result6.params[3] + \n",
    "                                            test_data_copy['RT_sem'].loc[hour]*result6.params[2] + \n",
    "                                            test_data_copy['LMP_DA'].loc[hour]*result6.params[1] + \n",
    "                                            result6.params[0])\n",
    "                \n",
    "        single_list.append(pe(test_data_copy['fitted RT (single)'], test_data_copy['LMP_RT']).mean())\n",
    "        six_list.append(pe(test_data_copy['fitted RT (6)'], test_data_copy['LMP_RT']).mean())\n",
    "\n",
    "        \n",
    "    single_array[i] = np.array(single_list).mean()\n",
    "    PE_6_array[i] = np.array(six_list).mean()\n",
    "    print('{}%'.format(10*i))\n",
    "    \n",
    "print('multi-cluster MAPE: {}'.format(PE_6_array.mean()))\n",
    "print('single-cluster MAPE: {}'.format(single_array.mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, won't group by month -- let's change the model\n",
    "# we'll have minute of the day, day of the week, day of the month and month as features\n",
    "# so we'll split the whole year's DA-RT into training and testing before any grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GMT MKT Interval</th>\n",
       "      <th>Coal_Mkt</th>\n",
       "      <th>Coal_Self</th>\n",
       "      <th>Diesel Fuel Oil Market</th>\n",
       "      <th>Diesel Fuel Oil Self</th>\n",
       "      <th>Hydro Market</th>\n",
       "      <th>Hydro Self</th>\n",
       "      <th>Natural Gas Market</th>\n",
       "      <th>Gas Self</th>\n",
       "      <th>Nuclear Market</th>\n",
       "      <th>...</th>\n",
       "      <th>LMP_DA</th>\n",
       "      <th>Interval_RT</th>\n",
       "      <th>LMP_RT</th>\n",
       "      <th>Local Date_price</th>\n",
       "      <th>Local Time</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Minute</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interval</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:05:00</th>\n",
       "      <td>2018-01-01T06:05:00Z</td>\n",
       "      <td>2917.3</td>\n",
       "      <td>18191.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>43.3</td>\n",
       "      <td>775.3</td>\n",
       "      <td>5842.2</td>\n",
       "      <td>3468.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01/01/2018 00:05:00</td>\n",
       "      <td>246.567610</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:05:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:10:00</th>\n",
       "      <td>2018-01-01T06:10:00Z</td>\n",
       "      <td>2916.5</td>\n",
       "      <td>18181.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>802.2</td>\n",
       "      <td>5983.2</td>\n",
       "      <td>3479.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01/01/2018 00:10:00</td>\n",
       "      <td>64.864068</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:10:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:15:00</th>\n",
       "      <td>2018-01-01T06:15:00Z</td>\n",
       "      <td>2913.0</td>\n",
       "      <td>18159.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>776.5</td>\n",
       "      <td>6116.0</td>\n",
       "      <td>3385.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01/01/2018 00:15:00</td>\n",
       "      <td>164.686048</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:20:00</th>\n",
       "      <td>2018-01-01T06:20:00Z</td>\n",
       "      <td>2911.5</td>\n",
       "      <td>18154.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>713.7</td>\n",
       "      <td>6182.6</td>\n",
       "      <td>3438.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01/01/2018 00:20:00</td>\n",
       "      <td>173.867912</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:20:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01/01/2018 00:25:00</th>\n",
       "      <td>2018-01-01T06:25:00Z</td>\n",
       "      <td>2895.3</td>\n",
       "      <td>18181.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.3</td>\n",
       "      <td>678.1</td>\n",
       "      <td>6184.3</td>\n",
       "      <td>3467.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>01/01/2018 00:25:00</td>\n",
       "      <td>163.353003</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>00:25:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         GMT MKT Interval  Coal_Mkt  Coal_Self  \\\n",
       "Interval                                                         \n",
       "01/01/2018 00:05:00  2018-01-01T06:05:00Z    2917.3    18191.4   \n",
       "01/01/2018 00:10:00  2018-01-01T06:10:00Z    2916.5    18181.4   \n",
       "01/01/2018 00:15:00  2018-01-01T06:15:00Z    2913.0    18159.0   \n",
       "01/01/2018 00:20:00  2018-01-01T06:20:00Z    2911.5    18154.1   \n",
       "01/01/2018 00:25:00  2018-01-01T06:25:00Z    2895.3    18181.1   \n",
       "\n",
       "                      Diesel Fuel Oil Market   Diesel Fuel Oil Self  \\\n",
       "Interval                                                              \n",
       "01/01/2018 00:05:00                      0.0                    4.4   \n",
       "01/01/2018 00:10:00                      0.0                    0.0   \n",
       "01/01/2018 00:15:00                      0.0                    0.0   \n",
       "01/01/2018 00:20:00                      0.0                    0.0   \n",
       "01/01/2018 00:25:00                      0.0                    0.0   \n",
       "\n",
       "                      Hydro Market   Hydro Self   Natural Gas Market  \\\n",
       "Interval                                                               \n",
       "01/01/2018 00:05:00           43.3        775.3               5842.2   \n",
       "01/01/2018 00:10:00           43.8        802.2               5983.2   \n",
       "01/01/2018 00:15:00           43.8        776.5               6116.0   \n",
       "01/01/2018 00:20:00           43.3        713.7               6182.6   \n",
       "01/01/2018 00:25:00           43.3        678.1               6184.3   \n",
       "\n",
       "                      Gas Self   Nuclear Market   ...    LMP_DA  \\\n",
       "Interval                                          ...             \n",
       "01/01/2018 00:05:00     3468.0              0.0   ...       0.0   \n",
       "01/01/2018 00:10:00     3479.6              0.0   ...       0.0   \n",
       "01/01/2018 00:15:00     3385.1              0.0   ...       0.0   \n",
       "01/01/2018 00:20:00     3438.3              0.0   ...       0.0   \n",
       "01/01/2018 00:25:00     3467.2              0.0   ...       0.0   \n",
       "\n",
       "                             Interval_RT      LMP_RT  Local Date_price  \\\n",
       "Interval                                                                 \n",
       "01/01/2018 00:05:00  01/01/2018 00:05:00  246.567610        2018-01-01   \n",
       "01/01/2018 00:10:00  01/01/2018 00:10:00   64.864068        2018-01-01   \n",
       "01/01/2018 00:15:00  01/01/2018 00:15:00  164.686048        2018-01-01   \n",
       "01/01/2018 00:20:00  01/01/2018 00:20:00  173.867912        2018-01-01   \n",
       "01/01/2018 00:25:00  01/01/2018 00:25:00  163.353003        2018-01-01   \n",
       "\n",
       "                     Local Time      Hour  Weekday  Month  Day  Minute  \n",
       "Interval                                                                \n",
       "01/01/2018 00:05:00    00:05:00  00:00:00     True      1    1       5  \n",
       "01/01/2018 00:10:00    00:10:00  00:00:00     True      1    1      10  \n",
       "01/01/2018 00:15:00    00:15:00  00:00:00     True      1    1      15  \n",
       "01/01/2018 00:20:00    00:20:00  00:00:00     True      1    1      20  \n",
       "01/01/2018 00:25:00    00:25:00  00:00:00     True      1    1      25  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DART_gen_2018 = DART_gen_2018.rename(columns={'Minute of Day':'Minute'})\n",
    "DART_gen_2018.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DART_gen_2018.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "PE_year_array = np.zeros(n)\n",
    "for i in range(n):\n",
    "    train, test = train_test_split(DART_gen_2018, test_size=0.33)\n",
    "    result = smf.ols(formula=\"LMP_RT ~ LMP_DA + Load + Wind + Month + Day + Minute + Weekday\", data=train).fit()\n",
    "    test['fitted RT (annual)'] = (\n",
    "                                  test['Minute']*result.params[7] +\n",
    "                                  test['Day']*result.params[6] +\n",
    "                                  test['Month']*result.params[5] +\n",
    "                                  test['Wind']*result.params[4] +\n",
    "                                  test['Load']*result.params[3] +\n",
    "                                  test['LMP_DA']*result.params[2] +\n",
    "                                  test['Weekday']*result.params[1] +\n",
    "                                  result.params[0])\n",
    "    PE_year_array[i] = pe(test['fitted RT (annual)'],test['LMP_RT']).mean()\n",
    "PE_year_array.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
